---
id: 19bb480feecdbd90
newsletter: turing-post
newsletter_name: "ğŸ”³ Turing Post"
category: tech-ai
subject: "FOD#135: What It Means When AI Labs Step Into Healthcare"
date: Mon, 12 Jan 2026 23:17:14 +0000 (UTC)
source_url: "https://www.turingpost.com/p/fod135"
---

# FOD#135: What It Means When AI Labs Step Into Healthcare

**From:** "ğŸ”³ Turing Post" <turingpost@mail.beehiiv.com>
**Date:** Mon, 12 Jan 2026 23:17:14 +0000 (UTC)
**Source:** [View original](https://www.turingpost.com/p/fod135)

---

## This Week in Turing Post:

* **Wednesday **/ AI 101 series: **Web World Models **

* **Friday** / We will start a **New Series!**

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

### ğŸ¤ From our partners: Vault-Free Privileged Access for Modern Engineering Teams

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/175d8b9c-c980-4e42-9419-72575a56dc42/image.png?t=1768254986)
Follow image link: (https://fandf.co/3MLWk60)
Caption: 

As AI and cloud infrastructure scale, managing privileged access with static credentials and vaults becomes both a bottleneck and a risk. Teleport replaces rotated credentials and vaulted secrets with real Zero Trust, issuing short-lived, cryptographic certificates at runtime for every human, machine, and AI agent.Â 

[Discover how vault-free PAM reduces risk and accelerates engineering.](https://fandf.co/3MLWk60)

Learn more (https://fandf.co/3MLWk60)

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

**Last week at CES:** Robots! More Robots! And Jensen Huang says they will have human-level capabilities THIS year. We went to see if robots were aware of that. [Watch the video :)](https://youtu.be/PAjb83HHLaU)

Youtube: Jensen Huang says robots will have human capabilities this year! The Robots at CES Had.. Other Plans (https://youtu.be/PAjb83HHLaU)

Also last week: **Why OpenAI and Anthropic Chose Healthcare at the Same Time**

Right after the holidays, both [OpenAI](https://openai.com/index/openai-for-healthcare/) and [Anthropic](https://www.anthropic.com/news/healthcare-life-sciences) announced healthcare-focused initiatives within days of each other. For the first time, I donâ€™t think about it as a competition, what I like about it is that itâ€™s a signal that **healthcare has crossed a threshold where staying out is no longer the cautious choice.** 

For several years, healthcare was treated as a deferred domain for leading AI labs. Understandably: the sector is heavily regulated, operationally fragmented, and unforgiving to confident mistakes. Earlier generations of models were difficult to bound, difficult to audit, and prone to failure modes that could not be cleanly isolated from their successes. In low-stakes domains, this was ok. In healthcare â€“ not at all.

The decision by both labs to move now implies a shared conclusion that something fundamental has changed. The **models are **for sure more capable now, but most importantly â€“ they are **more governable. **

**Healthcare is **therefore better understood as **a systems test **rather than a market opportunity. **This is a hugely important step in AI adoption.**

Another moment worth mentioning: doctors should not be worried. **What AI is being applied to is coordination. **Itâ€™s an old problem in healthcare that no one is structurally positioned to assemble full context under time pressure: information is distributed across multiple systems, and signals from medications, labs, imaging, wearables, genetics, and prior history are rarely considered together when decisions are made â€“ and patients are left to play detectives putting all the pieces together on their own. In this framing, **LLMs** are not making medical judgments. They **mainly help bring existing information together so it can be reviewed more easily**. 

Both labs appear to believe this coordination role is now stable enough to **turn into a product.**

**Where the two labs differ is in how they approach this coordination role.** 

**OpenAI is extending its general assistant** into healthcare, treating health data as another high-value context that can sit alongside documents, calendars, and enterprise tools, with additional privacy and access controls layered on top. The underlying assumption is that a single, familiar interface can serve patients, clinicians, and administrative workflows, as long as the boundaries around data use are clearly defined.

**Anthropic is taking a narrower approach.** Its healthcare effort is oriented less toward a patient-facing assistant and more toward embedding Claude inside existing institutional workflows. The emphasis is on predictable behavior, limited scope, and alignment with how healthcare organizations already operate. Rather than broad continuity across use cases, the focus is on fitting cleanly into specific professional contexts.

The choices what to focus on reflect different theories of how trust is built in regulated systems. One assumes trust emerges from continuity and widespread use, the other from constraint and institutional alignment. It is not yet clear which approach will prove more durable, and it is possible that both will coexist in different parts of the system. What matters is that both labs are now willing to test their models in an environment where responsibility cannot remain abstract. Iâ€™m very excited about this new development.

----------
_**Follow us on **__            _ğŸ¥_[ YouTube](https://www.youtube.com/@RealTuringPost)__              __[Twitter](https://x.com/TheTuringPost)__             __[ Hugging Face ](https://huggingface.co/Kseniase)_ğŸ¤—

----------â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

## Twitter Library 

11 New Interesting Policy Optimization Techniques: (https://www.turingpost.com/p/policyoptimization)

Upgrade (https://www.turingpost.com/upgrade)

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

### We are reading

* [On the Slow Death of Scaling](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5877662) by Sara Hooker

* [a16z: The Power Brokers](https://www.notboring.co/p/a16z-the-power-brokers)** **by Packy McCormick

* [Zhipu AI and MiniMax Just Went Public, But They're Not China's OpenAI](https://recodechinaai.substack.com/p/zhipu-ai-and-minimax-just-went-public) by Recode China

* [Inside MiniMax: Testing if AGI is Possible Without Infinite VC Money](https://www.turingpost.com/p/minimax)

### News from the usual suspects

* **Gmail Gets Gemini-fied**
Gmail is [stepping into](https://blog.google/products-and-platforms/products/gmail/gmail-is-entering-the-gemini-era/) 2026 with Gemini AI at the helm. Googleâ€™s flagship inbox now offers AI Overviews to summarize email threads, answer natural language queries, and filter clutter with the upcoming â€œAI Inbox.â€ Help Me Write and Suggested Replies get smarter, while proofreading goes premium. Itâ€™s no longer just email â€“ itâ€™s your AI-powered executive assistant.

* **Apple + Google: The Gemini Marriage**
Apple has [picked](https://www.cnbc.com/2026/01/12/apple-google-ai-siri-gemini.html?) Googleâ€™s Gemini to power the long-delayed AI upgrade to Siri, marking a rare alliance between rivals. The multiyear partnership puts Gemini models at the core of Appleâ€™s upcoming â€œFoundation Models,â€ keeping compute mostly on-device and in Appleâ€™s private cloud. Apple remains mum on the $1B/year price tag, but this move signals Cupertino is finally showing up to the AI arms race â€“ fashionably late, of course.

* **Musk's Macrohard Moment**
xAI, Elon Muskâ€™s AI venture, [torched $7.8 billion](https://www.bloomberg.com/news/articles/2026-01-09/musk-s-xai-reports-higher-quarterly-loss-plans-to-power-optimus) in just nine months, chasing its dream of powering humanoid robots like Optimus. Despite swelling quarterly losses, revenue doubled to $107 million, and a $20B cash injection (featuring Nvidia) suggests the spending spree is far from over. "Macrohard" may be a pun on Microsoft â€“ but the burn rate is no joke.

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

### ğŸ”¦ Research highlight

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/799cafea-4c15-48a0-a50b-3c612b854903/Screenshot_2026-01-12_at_4.33.23_PM.png?t=1768253614)
Caption: 

Researchers from MIT CSAIL present Recursive Language Models (RLMs), a novel inference-time architecture enabling LLMs to process arbitrarily long prompts â€“ scaling beyond 10 million tokens, over 100Ã— typical context windows. Instead of consuming the prompt directly, RLMs offload it into a Python REPL as a variable (`context`), allowing the LLM to symbolically interact with the prompt via code. The model can read, transform, and decompose the context and recursively call sub-LLMs through a built-in `llm_query()` function. This enables dynamic task decomposition, selective context access, and unbounded reasoning. RLMs require no retraining and work with existing models (GPT-5, Qwen3-Coder), achieving up to 2Ã— higher accuracy than base LLMs and long-context agents on benchmarks like BrowseComp+, OOLONG, and OOLONG-Pairs, while keeping inference cost comparable or lower. Ablation studies confirm the critical role of both the REPL environment and recursive sub-calls in solving complex, information-dense tasks.
**This is a significant step forward because RLMs break the fundamental context window barrier of LLMs â€“ enabling scalable, symbolic, and recursive reasoning over massive inputs without retraining or architectural changes** â†’[read the paper](https://arxiv.org/abs/2512.24601)

## Models

* **Liquid: LFM2.5 â€“ The Next Generation of On-Device AI**
Release an open-weight 1.2B-class model family optimized for edge agents by extending pretraining to 28T tokens, scaling post-training with multi-stage reinforcement learning, and shipping text, Japanese, vision-language, and native audio variants with day-zero runtime support across common inference stacks and NPUs [â†’read the paper](https://www.liquid.ai/blog/introducing-lfm2-5-the-next-generation-of-on-device-ai)

* **MiMo-V2-Flash Technical Report**
Deliver fast, strong reasoning and agentic performance by combining a large MoE backbone with hybrid attention, multi-token prediction, and multi-teacher on-policy distillation to push decoding speed and parameter efficiency [â†’read the paper](https://arxiv.org/abs/2601.02780) 

* **K-EXAONE Technical Report**
Provide a multilingual MoE foundation model with long-context support that targets balanced reasoning, agentic, and industrial capabilities across multiple major languages [â†’read the paper](https://arxiv.org/abs/2601.01739)

* **LTX-2: Efficient Joint Audio-Visual Foundation Model**
Generate temporally synchronized video and audio in a single unified model by coupling asymmetric modality-specific transformers through cross-attention for efficient, controllable audiovisual synthesis [â†’read the paper](https://arxiv.org/abs/2601.03233)

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

## Research this week 

_(_ğŸŒŸ_ indicates papers that we recommend to pay attention to)_

**World models, environments, and embodied learning**

* **Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models**
Unify how AI augments digital twins across modeling, mirroring, intervention, and autonomous management stages [â†’read the paper](https://arxiv.org/abs/2601.01321)

* ğŸŒŸ **WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks (Microsoft)**
Provide a large-scale, non-stationary web environment with rubric-based rewards to train and evaluate visual web agents [â†’read the paper](https://arxiv.org/abs/2601.02439)

* **Scaling Behavior Cloning Improves Causal Reasoning**
Show that scaling data and depth in behavior cloning improves causal policies in real-time video game agents [â†’read the paper](https://arxiv.org/abs/2601.04575)

* **Evolving Programmatic Skill Networks**
Grow a compositional network of executable skills that reflect, refactor, and stabilize over time in open-ended environments [â†’read the paper](https://arxiv.org/abs/2601.03509)

**Agents, tools, and orchestration**

* **Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning**
Route across models and tools using training-free priors and reinforcement learning to exploit heterogeneity in complex reasoning tasks [â†’read the paper](https://arxiv.org/abs/2601.03872)

* **MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning**
Interleave multimodal chain-of-thought reasoning with autonomous tool invocation to solve open-ended, real-world problems [â†’read the paper](https://arxiv.org/abs/2512.23412)

* **RelayLLM: Efficient Reasoning via Collaborative Decoding**
Coordinate small and large models at the token level so lightweight models request help only when needed to cut inference cost [â†’read the paper](https://arxiv.org/abs/2601.05167)

* ğŸŒŸ **Over-Searching in Search-Augmented Large Language Models (Apple)**
Diagnose when retrieval harms efficiency and truthfulness and propose metrics and mitigations for search overuse [â†’read the paper](https://arxiv.org/abs/2601.05503) â†’

* **Can We Predict Before Executing Machine Learning Agents?**
Replace costly execution with predictive reasoning by internalizing execution priors and using a predict-then-verify loop [â†’read the paper](https://arxiv.org/abs/2601.05930)

* **GenCtrl: A Formal Controllability Toolkit for Generative Models**
Formalize controllability as a control problem and estimate controllable sets to expose the limits of human influence over generation [â†’read the paper](https://arxiv.org/abs/2601.05637)

**Agent memory, long-horizon reasoning, and experience compression**

* **SimpleMem: Efficient Lifelong Memory for LLM Agents**
Compress interaction histories into high-density semantic memory units, consolidate them asynchronously into abstractions, and retrieve them adaptively to reduce token cost while preserving long-term performance [â†’read the paper](https://arxiv.org/abs/2601.02553)

* **MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents**
Represent memories across semantic, temporal, causal, and entity graphs and retrieve them via policy-guided traversal to enable interpretable, query-aligned long-horizon reasoning [â†’read the paper](https://arxiv.org/abs/2601.03236)

* **Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning**
Organize experiences into an event graph with explicit logical relations to support structured navigation over memory instead of shallow similarity search [â†’read the paper](https://arxiv.org/abs/2601.04726)

* **Distilling Feedback into Memory-as-a-Tool**
Amortize inference-time critique by storing feedback as retrievable guidelines that agents can reuse as a tool to reduce reasoning cost [â†’read the paper](https://arxiv.org/abs/2601.05960)

**Agent evaluation, verification, and confidence**

* **Agent-as-a-Judge**
Evolve evaluation from single-pass model judging to agentic judges with planning, tools, collaboration, and memory to enable verifiable multi-step assessment [â†’read the paper](https://arxiv.org/abs/2601.05111)

* **Agentic Rubrics as Contextual Verifiers for SWE Agents**
Generate repository-specific rubric checklists via agent interaction to verify code patches without executing tests while remaining grounded and interpretable [â†’read the paper](https://arxiv.org/abs/2601.04171)

* **Confidence Estimation for LLMs in Multi-turn Interactions**
Measure and improve confidence calibration across turns by formalizing monotonicity and per-turn reliability as context accumulates [â†’read the paper](https://arxiv.org/abs/2601.02179)

* **Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency**
Evaluate belief robustness by probing consistency across contextual neighborhoods rather than relying on point-wise self-consistency [â†’read the paper](https://arxiv.org/abs/2601.05905)

**Reasoning dynamics, structure, and control**

* **DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs**
Reformulate chain-of-thought generation as an iterative denoising process to enable retrospective correction of reasoning steps [â†’read the paper](https://arxiv.org/abs/2601.03559)

* **The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning**
Analyze long reasoning traces as structured interaction patterns and guide the synthesis of stable reasoning trajectories [â†’read the paper](https://arxiv.org/abs/2601.06002)

* **Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy**
Decompose large counting tasks into reliable subproblems and trace how intermediate counts are represented and aggregated inside the model [â†’read the paper](https://arxiv.org/abs/2601.02989)

* **Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners**
Probe how latent reasoning forms across languages and show that internal reasoning dynamics largely follow an English-centered pathway [â†’read the paper](https://arxiv.org/abs/2601.02996)

* **Parallel Latent Reasoning for Sequential Recommendation**
Scale reasoning width by exploring multiple latent reasoning trajectories in parallel to improve generalization under real-time constraints [â†’read the paper](https://arxiv.org/abs/2601.03153)

**Training efficiency, data efficiency, and optimization**

* **SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving**
Push lightweight supervised fine-tuning to state-of-the-art SWE performance through curated datasets, curriculum design, and verifier-based test-time scaling [â†’read the paper](https://arxiv.org/abs/2601.01426)

* **One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling**
Demonstrate that a single, carefully engineered training sample can unlock broad reasoning gains across domains via reinforcement learning â†’[read the paper](https://arxiv.org/abs/2601.03111)

* **Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting**
Suppress destructive gradients on confident-but-conflicting tokens by gating updates with entropy to reduce catastrophic forgetting during fine-tuning [â†’read the paper](https://arxiv.org/abs/2601.02151)

* **Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers**
Replace fixed norm equilibria with learnable scaling factors to adapt weight magnitudes to data and improve downstream performance [â†’read the paper](https://arxiv.org/abs/2601.04890)

* ğŸŒŸ **GDPO: Group reward-Decoupled Normalization Policy Optimization (Nvidia) **
Decouple reward normalization in multi-reward reinforcement learning to preserve signal resolution and improve training stability [â†’read the paper](https://arxiv.org/abs/2601.05242)

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

_Thatâ€™s all for today. Thank you for reading! Please __**send this newsletter to colleagues**__ if it can help them enhance their understanding of AI and stay ahead of the curve._

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

Share Turing Post

You currently have <strong>0</strong> referrals, only <strong>3</strong> away from receiving <strong>1 Month of Premium Subscription</strong>.

Or copy and paste this link to others: https://www.turingpost.com/subscribe?ref=LYY3SYS432

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

â€”â€”â€”

You are reading a plain text version of this post. For the best experience, copy and paste this link in your browser to view the post online:
https://www.turingpost.com/p/fod135
