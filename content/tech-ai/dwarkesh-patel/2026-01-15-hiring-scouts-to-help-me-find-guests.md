---
id: 19bc287a0029ecfe
newsletter: dwarkesh-patel
newsletter_name: "Dwarkesh Patel"
category: tech-ai
subject: "Hiring scouts to help me find guests"
date: Thu, 15 Jan 2026 16:40:00 +0000
source_url: "https://www.dwarkesh.com/p/hiring-scouts-to-help-me-find-guests"
---

# Hiring scouts to help me find guests

**From:** Dwarkesh Patel <dwarkesh+blog@substack.com>
**Date:** Thu, 15 Jan 2026 16:40:00 +0000
**Source:** [View original](https://www.dwarkesh.com/p/hiring-scouts-to-help-me-find-guests)

---

View this post on the web at https://www.dwarkesh.com/p/hiring-scouts-to-help-me-find-guests

My main bottleneck is finding excellent guests. So, I’m hiring a couple part time scouts to help me find the next David Reich/Sarah Paine/Adam Brown.
$100/hour, fully remote, work hours are flexible - I expect it’ll be 5-10 hours a week.
Ideal candidate is maybe a grad student, or a post doc, or working in one of the fields I wanna find guests in. I’m looking for people who are really plugged into some discipline and have high taste.
Beyond just scouting guests, I’ll want your help assembling curriculums that help me prep for interviews and rapidly get up to speed.
The application form is here , and it’s extremely simple - just pitch me on a guest and tell me a bit about yourself. Please submit by 11:59 PM Pacific, Friday, Jan 23.
I’m looking to hire ~one scout for each of the following fields: bio, history, econ, math/physics, AI/hardware.
However, it’s very possible I end up hiring more (or fewer), or break apart the domains of knowledge in a different way, based on the range of expertise of the best people who apply.
What I’m looking for in guests
I’m looking for people who are deep experts in at least one field, and who are polymathic enough to think through all kinds of tangential questions in a really interesting way. 
So I’m selecting for this synthetic ability to connect one’s expertise to all kinds of important questions about the world - an ability which is often deliberately masked in public academic work. Which means that it can only really come out in conversation.
That’s why I want to hire scouts. I need their network and context - they know who the polymathic geniuses are, who gave a fascinating lecture at the last big conference they attended, who can just connect all kinds of interesting ideas in the field together over conversation, etc.
We get tons of inbound from people who are working on impressive companies or doing interesting research projects. But almost always it’s a no; while I think their work is important, it’s self-contained in a way that I worry won’t lead to interesting broad discussion.
To get a little more concrete, here’s what worked well about some of my recent favorite guests:
Let me talk through why I think some interviews worked especially well, so you can think about what people in fields you’re familiar with fill a similar mold.
Jacob Kimmel : A lot of people who pitch themselves as guests are capable of only talking about their own research. But the amazing thing about Jacob is that he is an insane polymath. For example, he could explain why evolution didn’t select for longevity by drawing deep analogies to how gradients flow in ML models. He had all these other random interesting takes, from why humans never evolved their own antibiotics to how there’s this gene that used to protect us from HIV-like viruses but got repurposed, which hints at some ghost scourge. And then he could zoom out and give a great diagnosis of what’s bottlenecking pharma progress. I really want to emphasize how that’s different from other brilliant people I get pitched – these people are also doing incredible research, but they don’t have this range of really deep, interesting takes. That part is super crucial.
David Reich : It’s actually quite surprising that my second most popular guest of all time is a geneticist of ancient DNA. How did that happen? Here’s why I think this episode blew up. In high school, you get some vague explanation of human evolution. And you feel like you understand it and can move on with your life. And here comes David, showing you how this very fundamental topic, which you assume was settled and haven’t bothered thinking about in years, is actually way more murky and surprising than you realized, and how new discoveries are totally overturning our basic understanding of the field (in this case, the how, when, where of human evolution).
Andrej Karpathy : It’s extremely rare to get someone who is expert-level in a technical, fast-moving, and frothy field, but who has no vested interest in a particular company or approach, and who is in a position to just give an unbiased lay of the land. I have a couple questions below about biotech or formal math or robotics in the wake of AI progress - if there’s a Karpathy-type person in those fields, I’d be very keen to get a technical lay of the land and vibe check of what claims are credible versus crazy.
Some recent questions
In case it’s helpful for brainstorming a guest, I’ve listed out a few big questions that have been on my mind recently. But please feel free to ignore them - there’s way more interesting questions in the world than the ones I am aware of - feel free to say, “You might not yet be curious about antibody development/the history of language/the dark ages/battery tech, but the guest I have in mind for that topic is so good that it’s going to be your next big banger episode.”
Bio
Dario’s Machines of Loving Grace  argues we’ll compress a century of bio progress into a few years - that big breakthroughs like CAR-T therapy, mRNA vaccines, cheap genome sequencing, etc  show how in the long run things which seem like data or physical bottlenecks can be solved by better tools to measure/predict/perturb/understand biological system, and these tools are downstream of intelligence. But here’s what I don’t fully understand: over the last 3 decades, we’ve seen a million-fold reduction in genome sequencing costs, 1000-fold decrease in DNA synthesis costs, the development of precise gene editing tools like CRISPR, and the ability to conduct massively parallel experiments through multiplexing techniques. But it doesn’t seem like we’re curing diseases or coming up with new treatments at a faster rate now than we were 30 years ago. If anything, drug development is slowing down . I want to find a biology researcher who can think through how plausible a 10x or 100x speedup in new drug discovery actually is. They should obviously know a lot about and have hot takes on what’s actually bottlenecking progress today, and they should be flexible enough to imagine what might change with much more intelligence.
What exactly is the special sauce of the brain that we’re still missing? Adam Marblestone thinks  it’s the curriculum of reward functions and the learning/steering subsystems. Others argue that gradient descent is fundamentally worse than how the brain learns within a lifetime (which is closer to in-context learning in its flexibility and sample efficiency).
Math/Physics
I’ve been really enjoying Strogatz’s Nonlinear Dynamics and Chaos  textbook, and I want to make something podcast-shaped out of it. Strogatz himself has deferred until after he finishes his next book, so I’m looking for another mathematician on a related topic. I think the right format here isn’t a normal meandering interview - it’s something more like a lecture. A mathematician comes in with a specific topic or example we can deep dive on. He posts up at a blackboard, starts explaining a topic, and I interrupt to clarify confusions and ask follow-up questions. The model is something like Terence Tao and Grant Sanderson’s cosmic distance ladder video . Who can replicate something similar with me with some independently explainable topic in chaos/nonlinear dynamics or adjacent topics? I’d be especially keen if someone can present something on how the topics in this textbook tie into ML (see for example Neural network training makes beautiful fractals ).
What real world impact should we expect from the current batch of AI for math projects? What are the fields of technology where people are going, “Ah we could totally solve quantum computing (or fusion or AGI) only if we had more theorems!” But maybe problems in biology and physics and materials and so on reduce down to math in a way I’m not foreseeing, and automating formal math alone is enough to unlock a bunch of progress. See footnotes for some more questions I wanna ask the right guest on this topic.
I started reading Proofs and Refutations , which is this famous 1976 book by the Hungarian mathematician Imre Lakatos about the philosophy of mathematics. He says math involves a lot of changing definitions and swapping lemmas in order to deal with different counterexamples. This seems fine for a good faith mathematical community, but super reward hackable for these AI-for-math models. Also it involves a lot of realizing how a problem in one domain is really a problem in another, and noticing the meta level pattern - AIs so far have been especially bad at this kind of thing. If math is just proof search within a fixed formal system, then AI can help a lot. But if its dialectical construction and refinement of concepts (based on what tasteful parsimonious definition can withstand counterexamples) , then I feel self play and ‘automated cleverness’ alone won’t do the trick. But maybe automated counterexamples are super useful. I’m sure for practicing mathematicians there’s a bunch of stuff that’s naive or wrong about the above. Would love to chat out what the actual research math process is like, and what good it would do to automate it.
AI/hardware
RL progress has been very fast, but it’s partly the result of going from almost nothing to 1e26 FLOPs training compute in a year (aka like going from GPT-1 to GPT-4.5). It’s still possible that it has terrible scaling exponents and further progress will be very slow. And also it’s not clear how much of the progress over the last year comes from inference scaling, which has worse variable economics. But on the other hand, maybe there’s a ton of low hanging fruit in improving RL - with pretraining, there’s been 5 years of developing the theory and empirics of optimal batch sizes, learning rates, architectures, etc. As that low hanging fruit is picked, maybe RL progress continues to be fast? The other big question about RL training is how much transfer learning are we seeing - is there all this crazy meta learning that’s not directly induced by any env and which will enable flexible human-like labor soon? I have no idea. My friends at labs who are actually doing this training obviously wouldn’t tell me. But I want to actually concretely understand what’s going on here.
History
There’s the famous Needham question , which asks why China didn’t industrialize first despite leading the world in population, inventions, and bureaucratic sophistication. I find the standard explanation of how this centralized Ming/Qing regime damped invention and exploration unsatisfying. Or at least I don’t understand it concretely. It’s such a big country - how can you retard progress across the whole thing, especially given that state capacity was presumably weaker in the past? Or at least I assume it was - what did a provincial bureaucrat actually do day-to-day? Was there a price system? Private property? How did the state actually interfere with merchants and artisans?
Economics
There’s something unsatisfying about the arguments  that we’ll see 20%+ explosive economic growth from AI. Even if true, what does that mean? What is actually happening? I thought Machines of Loving Grace  was a great account of what plausibly is happening on the human facing side of the singularity - aka the FLOPs that are going towards curing disease. But presumably most of what is happening is investment towards more robots, more compute, etc. My sense of what that side of things looks like is so murky and handwavy. There is a version of Machines of Loving Grace you can do that is somewhat concrete about all the sci fi shit - not just gesturing at the galaxies, but getting specific about the space GPUs and factorio like solar tiling and all the other things I’m not thinking of which are relevant to understanding 2040. Presumably the right guest is someone who is really strong in engineering/physics and economics and has a penchant for sci-fi and has a lot of concrete ideas here.
What should India or Nigeria or for that matter any country not directly in the semiconductor/foundation model supply chain do right now? If the main mechanism of catchup growth goes away (namely, that the underutilized labor of developing countries can rapidly be made more productive with capital and know-how from the developed world), what happens to all these countries that are not China or the US?

Unsubscribe https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly93d3cuZHdhcmtlc2guY29tL2FjdGlvbi9kaXNhYmxlX2VtYWlsP3Rva2VuPWV5SjFjMlZ5WDJsa0lqb3lOVE00TVRReUxDSndiM04wWDJsa0lqb3hPRFExTnpjMk9UZ3NJbWxoZENJNk1UYzJPRFE1TlRJeU5Td2laWGh3SWpveE9EQXdNRE14TWpJMUxDSnBjM01pT2lKd2RXSXROamt6TkRVaUxDSnpkV0lpT2lKa2FYTmhZbXhsWDJWdFlXbHNJbjAueFBmbF9sa2Mtd1hrUHFqck9weUk3RzdOaldBanYwUlpLelJ2UnBtQWt4OCIsInAiOjE4NDU3NzY5OCwicyI6NjkzNDUsImYiOnRydWUsInUiOjI1MzgxNDIsImlhdCI6MTc2ODQ5NTIyNSwiZXhwIjoyMDg0MDcxMjI1LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.Ep--3mJd5S-g_BkXbewFmRJhlTfQD_ZaKhOA7U8BbU8?
