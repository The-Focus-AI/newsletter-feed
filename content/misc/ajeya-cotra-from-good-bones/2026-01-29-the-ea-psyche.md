---
id: 19c0a4aabc534a09
newsletter: ajeya-cotra-from-good-bones
newsletter_name: "Ajeya Cotra from Good Bones"
category: misc
subject: "The EA psyche"
date: Thu, 29 Jan 2026 15:02:41 +0000
source_url: "https://acotra.substack.com/p/the-ea-psyche"
---

# The EA psyche

**From:** Ajeya Cotra from Good Bones <acotra@substack.com>
**Date:** Thu, 29 Jan 2026 15:02:41 +0000
**Source:** [View original](https://acotra.substack.com/p/the-ea-psyche)

---

View this post on the web at https://acotra.substack.com/p/the-ea-psyche

My first post  got a frankly bewildering number of likes and shares — thank you and welcome to all my new subscribers! Since many of you don’t know me at all, let me take a second to introduce myself. In my day job I work on trying to understand and track extreme risks from AI agents at METR . Before that, I spent almost a decade at Coefficient Giving . I write about AI futurism over at Planned Obsolescence , and this is my personal blog.
A lot of this blog will be, directly or indirectly, about effective altruism  (EA). I’ve been a pretty hardcore EA for over 15 years, since I was a high school freshman. In college I co-founded the EA Berkeley student club and co-taught a class about EA. In 2016, our club put on the first EAGxBerkeley, where I gave an intro to EA talk  that’s still on the EA resources page  and is sometimes featured in intro to EA courses and seminars ten years later.
A lot of EAs are the type who don’t like labels, who like to “keep their identity small,” who are always “adjacent” to something. I think they’re usually being too precious. I like labels. I like how I can say “I’m an EA, a rationalist, and a woman” and this short sentence compresses me so efficiently. And I like wearing the label and giving talks at EA Globals because I think EA is really great and way more people should get into it.
I also like that when you have a label for what you are, you can more easily look in on the thing-you-are from the outside. I recommend Ozy’s excellent post series  for an anthropological guide to what EAs actually believe. I think most of the more specific beliefs he talks about are downstream of two core EA psychological traits:
Taking ideas seriously . Most people don’t change anything significant about how they act after perceiving a pattern of squiggles on a screen. If they encounter arguments like the drowning child thought experiment  or the Simulation hypothesis  (which they probably won’t), they go “huh” and move on with their day. EAs are very unusually willing to change our lives around based on explicit logical arguments. We are very unusually unattached to the intuitions we’ve picked up about what’s “normal” and the “done thing,” and unusually likely to bet that first-principles reasoning can beat “expert consensus.” We’re more likely to do Lifecycle Investing  or buy AI stocks on the basis of arguments about the technologies, we were more likely to stock up on PPE (and short global markets) in January and February 2020, we’re more likely to be intensively analytical about our personal lives (see dating docs ), we’re more likely to be early adopters of novel medical interventions like GLP-1s or a genetically engineered bacterium that prevents cavities .
Radical empathy . EAs strive not to care about some beings more than others just because they are closer to us in space or time, just because we share a country or race or species or other group membership with them, or for any purely “aesthetic” reason (even as we debate what counts as “aesthetic” vs “substantive”). We strive not to care more about Americans than about Ugandans or Iranians or Chinese. We strive not to care more about pandas than pigs. We strive not to care more about humans than non-human animals except insofar as we have arguments that humans have a greater capacity for well-being or suffering (however we define that). We strive not to care more about sentient life in this universe than life in other universes. We strive not to care more about biological creatures than we do about digital minds , including the AIs we’re building now. This is not natural, it takes continuous effort to exercise the kind of discipline and imagination needed to act in accordance with radical empathy.
Radical empathy doesn’t necessarily follow from taking ideas seriously or vice versa, but in actual humans, they are very correlated traits. They also structurally amplify one another. No one is born caring about the Tegmark IV multiverse , so you have to argue yourself into it (i.e., taking ideas seriously generates radical empathy). And if you do decide you care about Tegmark IV then the only path to try to directly impact the thing you care about goes through abstract analysis, you can’t imitate what worked well for others or learn by trial and error (i.e., radical empathy forces taking ideas seriously).
If you plot the two traits against one another, there’s a region of the graph — “aiming to directly benefit exotic distant beings while making decisions using intuitive learned heuristics” — that’s impossible to reach.
The further you go to the top right of this graph, the more you’ll code as “EA” to fans and haters alike. I was born very EA, and spending over a decade in the heart of the community has made me even more EA.
But I’m definitely not maximally EA (I mean obviously no one is, but I regularly talk to people distinctly more EA than me on these axes). I’ve always been more intuitively uncomfortable with purely philosophical thinking leading to crazytown conclusions  than many of the people I hang out with every day. And the deeper I go in my career, the more salient it becomes to me that in the domains they can access, imitation learning and RL perform so much better than abstract argument.
Even in a whole career dedicated to trying to fill the lightcone with goodness, most of your learning and growing has to come from imitation and RL. That’s just how it works to build skills. This sucks for you if your goal is to positively shape the fate of the Tegmark IV multiverse, because you’ll always have vast uncertainty about whether the feedback signals you have available to train on (people reading your posts or passing your policies) are all that correlated with the ultimate unobservable goal. Someone whose goal in life is to make as much money as possible gets the luxury of learning how to do their thing much more efficiently than you can learn how to do your thing.
In fact, I’m selecting my goals to be ones that have better feedback loops. My reluctance to rely more heavily on abstract argument is constraining my ability to practically go further on radical empathy. When you’re mostly uninvested in trying to directly influence X, it becomes a more academic question how much you care about X. In some sense I care a lot about helping beings outside the simulation we might be in (probably are in?) right now, or about making acausal trade  with distant parts of the multiverse go well, but these considerations don’t really enter into my decisions on a practical level. Instead, I gravitate to stuff like trying to help people understand where AI might be in six months.
And even though I think EA is really great, it’s a high-variance way of being. When someone decides to take ideas seriously and ambitiously shape the world according to them, the rest of us suddenly become intensely invested in the exact particulars of those ideas. Many hardcore EAs found their way to EA after first having been hardcore about Christianity. And though some of my friends might protest, I’d bet today’s hardcore EAs would have been disproportionately likely to be hardcore Marxists during the Great Depression.
When you get to the heady zone in the top right of the graph, I’m feeling pretty nervous unless you’ve cultivated a thousand more delicate qualities — carefulness, humility, curiosity, self-awareness, humor, judgment, patience, taste — alongside the core traits that define EAness.
In fact, I think most hardcore EAs do have these other little virtues. My friends and mentors are deeply good people, in a rich and full sense, and that’s what makes it work for them to be crazy EAs. In my view, the EA subculture we’re embedded in encodes some genuine wisdom about what it takes to be super hardcore — to actually work on space governance or acausal trade — with grace and, well, effectiveness.

Unsubscribe https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9hY290cmEuc3Vic3RhY2suY29tL2FjdGlvbi9kaXNhYmxlX2VtYWlsP3Rva2VuPWV5SjFjMlZ5WDJsa0lqb3lOVE00TVRReUxDSndiM04wWDJsa0lqb3hPRFl4TmpNM016Y3NJbWxoZENJNk1UYzJPVFk1T1RFNU1Dd2laWGh3SWpveE9EQXhNak0xTVRrd0xDSnBjM01pT2lKd2RXSXROek0wTVRJNE5pSXNJbk4xWWlJNkltUnBjMkZpYkdWZlpXMWhhV3dpZlEuRzZTcl8xY05DWHZCajBwbmk2em92ZS1fVERudldJSUFIMDhPM0VudVNUbyIsInAiOjE4NjE2MzczNywicyI6NzM0MTI4NiwiZiI6dHJ1ZSwidSI6MjUzODE0MiwiaWF0IjoxNzY5Njk5MTkwLCJleHAiOjIwODUyNzUxOTAsImlzcyI6InB1Yi0wIiwic3ViIjoibGluay1yZWRpcmVjdCJ9.rDdjSq6tWpr7ubCDz_3UfGKr52qK4KWnv5CL5yt8rL4?
